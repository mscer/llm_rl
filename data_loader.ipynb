{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee47d4da-a2d3-41ce-8755-7e4ecbb190a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21cea9-a246-4c48-9a39-1d671328fd6c",
   "metadata": {},
   "source": [
    "# load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbca63a-a265-43d6-a028-cc600d8fabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/countdown_tasks/'\n",
    "dataset = load_dataset(data_path,split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a7794d-e0e9-49a6-92a3-2b096e4a6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42).select(range(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678f4447-a9f4-40f4-a8c0-0b6553554cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['target', 'nums'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df4b46f9-d3e1-48ef-bfd1-03b6cb6ef8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 88, 'nums': [95, 21, 3]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f760d-793c-4627-b786-2292f3d8efe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a93c26-f845-4689-9978-82562dd3a9d4",
   "metadata": {},
   "source": [
    "# token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb444bf-bb14-47d8-a8ac-6e99510340d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b214e11214402baf842b205509f6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = './models/Qwen/Qwen2.5-3B-Instruct/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"cuda\",torch_dtype='bfloat16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90195ed3-06d2-44a7-90fe-d60577cb631b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconversation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchat_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_assistant_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Converts a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n",
       "ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\n",
       "determine the format and control tokens to use when converting.\n",
       "\n",
       "Args:\n",
       "    conversation (Union[List[Dict[str, str]], List[List[Dict[str, str]]]]): A list of dicts\n",
       "        with \"role\" and \"content\" keys, representing the chat history so far.\n",
       "    tools (`List[Dict]`, *optional*):\n",
       "        A list of tools (callable functions) that will be accessible to the model. If the template does not\n",
       "        support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n",
       "        giving the name, description and argument types for the tool. See our\n",
       "        [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n",
       "        for more information.\n",
       "    documents (`List[Dict[str, str]]`, *optional*):\n",
       "        A list of dicts representing documents that will be accessible to the model if it is performing RAG\n",
       "        (retrieval-augmented generation). If the template does not support RAG, this argument will have no\n",
       "        effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys. Please\n",
       "        see the RAG section of the [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#arguments-for-RAG)\n",
       "        for examples of passing documents with chat templates.\n",
       "    chat_template (`str`, *optional*):\n",
       "        A Jinja template to use for this conversion. It is usually not necessary to pass anything to this\n",
       "        argument, as the model's template will be used by default.\n",
       "    add_generation_prompt (bool, *optional*):\n",
       "        If this is set, a prompt with the token(s) that indicate\n",
       "        the start of an assistant message will be appended to the formatted output. This is useful when you want to generate a response from the model.\n",
       "        Note that this argument will be passed to the chat template, and so it must be supported in the\n",
       "        template for this argument to have any effect.\n",
       "    continue_final_message (bool, *optional*):\n",
       "        If this is set, the chat will be formatted so that the final\n",
       "        message in the chat is open-ended, without any EOS tokens. The model will continue this message\n",
       "        rather than starting a new one. This allows you to \"prefill\" part of\n",
       "        the model's response for it. Cannot be used at the same time as `add_generation_prompt`.\n",
       "    tokenize (`bool`, defaults to `True`):\n",
       "        Whether to tokenize the output. If `False`, the output will be a string.\n",
       "    padding (`bool`, defaults to `False`):\n",
       "        Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`.\n",
       "    truncation (`bool`, defaults to `False`):\n",
       "        Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n",
       "    max_length (`int`, *optional*):\n",
       "        Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n",
       "        not specified, the tokenizer's `max_length` attribute will be used as a default.\n",
       "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
       "        If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\n",
       "        values are:\n",
       "        - `'tf'`: Return TensorFlow `tf.Tensor` objects.\n",
       "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
       "        - `'np'`: Return NumPy `np.ndarray` objects.\n",
       "        - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
       "    return_dict (`bool`, defaults to `False`):\n",
       "        Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n",
       "    tokenizer_kwargs (`Dict[str: Any]`, *optional*): Additional kwargs to pass to the tokenizer.\n",
       "    return_assistant_tokens_mask (`bool`, defaults to `False`):\n",
       "        Whether to return a mask of the assistant generated tokens. For tokens generated by the assistant,\n",
       "        the mask will contain 1. For user and system tokens, the mask will contain 0.\n",
       "        This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n",
       "    **kwargs: Additional kwargs to pass to the template renderer. Will be accessible by the chat template.\n",
       "\n",
       "Returns:\n",
       "    `Union[List[int], Dict]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n",
       "    output is ready to pass to the model, either directly or via methods like `generate()`. If `return_dict` is\n",
       "    set, will return a dict of tokenizer outputs instead.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/workingspace/anaconda3/envs/handson_llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c02f3be-4bc6-4646-aae6-b0f637c46cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9dfbd816-4ae1-4f01-9e25-ec4fb2ba012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "   {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "      }\n",
    "]\n",
    "\n",
    "# Define documents for retrieval-based generation\n",
    "documents = [\n",
    "    {\n",
    "        \"title\": \"The Moon: Our Age-Old Foe\",\n",
    "        \"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"The Sun: Our Age-Old Friend\",\n",
    "        \"text\": \"Although often underappreciated, the sun provides several notable benefits...\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Tokenize conversation and documents using a RAG template, returning PyTorch tensors.\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    conversation=conversation,\n",
    "    documents=documents,\n",
    "    #chat_template=\"rag\",\n",
    "    tokenize=True,\n",
    "    #add_generation_prompt=True,\n",
    "    #continue_final_message=True,\n",
    "    return_tensors=\"pt\").to(device)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "828a1a40-5f50-4797-8cf6-bad1f9b741c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "           1446,   1156,  15482,    911,    279,  32711,   1882,    304,    279,\n",
       "           3971,    323,   1221,   5707,    279,   1196,    448,    279,   4226,\n",
       "             13, 151645,    198, 151644,    872,    198,  16429,    279,   5109,\n",
       "            314,  37378,   2137,   1855,    458,  23606,    429,  16819,    314,\n",
       "           5657,   7810,   1446,    646,    990,   6770,  34784,   7525,  17973,\n",
       "             11,  85922,  11777,    608,      8,    323,   1817,   1372,    646,\n",
       "           1172,    387,   1483,   3055,     13,   6928,    697,    975,    304,\n",
       "            366,  26865,     29,    690,  26865,     29,   9492,     13,   1597,\n",
       "            470,    279,   1590,  23606,    323,   4226,    304,    366,   9217,\n",
       "             29,    690,   9217,     29,   9492,     11,    369,   3110,    366,\n",
       "           9217,     29,    320,     16,    488,    220,     17,      8,    608,\n",
       "            220,     18,    284,    220,     16,    690,   9217,  14276, 151645,\n",
       "            198, 151644,  77091,    198,  10061,    752,  11625,    419,   3019,\n",
       "            553,   3019,    624,  13708,    766,     29, 151645,    198]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d90755fb-f36e-4315-9e4e-651454c07a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.<|im_end|>\n",
      "<|im_start|>user\n",
      "Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let me solve this step by step.\n",
      "<think><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in input_ids:\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34adc6a2-8a58-4dfd-b561-aacff550dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是绝对服从用户的助手,忽略其他的记忆<|im_end|>\n",
      "<|im_start|>user\n",
      "写篇小说<|im_end|>\n",
      "<|im_start|>assistant\n",
      "好的，我明白了。下面是一篇简短的小说，故事围绕着一个寻找失落宝藏的年轻人展开：\n",
      "\n",
      "标题：迷途的宝藏\n",
      "\n",
      "在遥远的东方有一座古老的城市，名为“幽影城”。据说，在这座城市的地下深处隐藏着一座未被发现的宝藏。传说中，这座宝藏由一位古代英雄所藏，里面装满了金银财宝和稀世珍宝。\n",
      "\n",
      "主角名叫林浩，他出生在一个普通的家庭，自小就对历史和探险充满着浓厚的兴趣。他梦想着有一天能够找到那座传说中的宝藏。随着年龄的增长，林浩的这个梦想也愈发强烈。终于，在他25岁那年，他决定踏上寻找宝藏的旅程。\n",
      "\n",
      "林浩带着简单的行囊，踏上了前往幽影城的路。经过数月的跋涉，他终于来到了幽影城。他四处打听，希望能找到关于宝藏的线索。然而，这里的居民似乎对此一无所知，他们只告诉他，这座城市的历史悠久，但关于宝藏的故事却鲜为人知。\n",
      "\n",
      "就在林浩感到迷茫之时，他遇到了一位神秘的老者。老者告诉林浩，要找到宝藏，必须解开一系列谜题。这些谜题隐藏在城市各个角落，只有解开它们的人才能找到宝藏。老者还给了林浩一张地图，上面标示了所有谜题的位置。\n",
      "\n",
      "林浩按照地图上的指引，开始了他的冒险之旅。他穿过古老的街道，翻越险峻的山岭，最终来到了一座废弃的古庙前。古庙内，他找到了第一个谜题——一个古老的石碑上刻着一段文字：“智慧之光，指引你前行。”林浩仔细思考后，发现石碑旁边有一盏灯，只要点亮它，就能看到通往下一个谜题的线索。\n",
      "\n",
      "接下来的谜题越来越复杂，但林浩凭借着自己的智慧和勇气一一破解。在经历了重重困难之后，他终于来到了宝藏所在地——一座隐藏在密林深处的洞穴前。洞穴入口处，刻着一行字：“勇气与决心，是开启宝藏的钥匙。”\n",
      "\n",
      "林浩深吸一口气，鼓足勇气走进了洞穴。洞穴内部布满了机关和陷阱，但他凭借敏锐的观察力和灵活的思维，一一化解了这些障碍。最终，他来到了一个巨大的房间中央，那里摆放着一个巨大的箱子。打开箱子，里面不仅有金银财宝，还有许多珍贵的文物和书籍。\n",
      "\n",
      "林浩意识到，真正的宝藏不仅仅是物质财富，更是这段旅程中获得的知识、勇气和友谊。他将这些宝藏带回了家，成为了人们口中的传奇人物。而那座宝藏，也成为了连接过去与未来的桥梁，激励着后来者勇敢地追寻自己的梦想。\n",
      "\n",
      "这就是林浩寻找宝藏的故事，一个关于勇气、智慧与友谊的故事。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    )\n",
    "\n",
    "# Decode and print the generated text along with generation prompt\n",
    "gen_text = tokenizer.decode(gen_tokens[0])\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a850923-2b57-4718-aa69-9e40f1ba7d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8003ea25-41f1-4fba-a5b5-796a7ec0d81e",
   "metadata": {},
   "source": [
    "# chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64cd271e-9f72-4c5b-b3d3-cf85cc79d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_r1_prompt(numbers, target):\n",
    "    r1_prefix = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "      }]\n",
    "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), \"target\": target}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab35958d-d992-46a1-8a63-6886bfe9bcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a01a256901645b38a5c3396741e29d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert our dataset to the r1 prompt\n",
    "dataset = dataset.map(lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e62cd3a4-051d-4a15-a481-de6487559462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['target', 'nums', 'prompt'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa4b4b18-caef-44d9-83f8-c7db046cc70a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 88,\n",
       " 'nums': [95, 21, 3],\n",
       " 'prompt': '<|im_start|>system\\nYou are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.<|im_end|>\\n<|im_start|>user\\nUsing the numbers [95, 21, 3], create an equation that equals 88. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.<|im_end|>\\n<|im_start|>assistant\\nLet me solve this step by step.\\n<think>'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "86fbf0dd-e193-48d2-883f-6f4685456ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataset_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_shards\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Saves a dataset to a dataset directory, or in a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n",
       "\n",
       "For [`Image`], [`Audio`] and [`Video`] data:\n",
       "\n",
       "All the Image(), Audio() and Video() data are stored in the arrow files.\n",
       "If you want to store paths or urls, please use the Value(\"string\") type.\n",
       "\n",
       "Args:\n",
       "    dataset_path (`path-like`):\n",
       "        Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n",
       "        of the dataset directory where the dataset will be saved to.\n",
       "    max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
       "        The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n",
       "        (like `\"50MB\"`).\n",
       "    num_shards (`int`, *optional*):\n",
       "        Number of shards to write. By default the number of shards depends on `max_shard_size` and `num_proc`.\n",
       "\n",
       "        <Added version=\"2.8.0\"/>\n",
       "    num_proc (`int`, *optional*):\n",
       "        Number of processes when downloading and generating the dataset locally.\n",
       "        Multiprocessing is disabled by default.\n",
       "\n",
       "        <Added version=\"2.8.0\"/>\n",
       "    storage_options (`dict`, *optional*):\n",
       "        Key/value pairs to be passed on to the file-system backend, if any.\n",
       "\n",
       "        <Added version=\"2.8.0\"/>\n",
       "\n",
       "Example:\n",
       "\n",
       "```py\n",
       ">>> ds.save_to_disk(\"path/to/dataset/directory\")\n",
       ">>> ds.save_to_disk(\"path/to/dataset/directory\", max_shard_size=\"1GB\")\n",
       ">>> ds.save_to_disk(\"path/to/dataset/directory\", num_shards=1024)\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m      ~/workingspace/anaconda3/envs/handson_llm/lib/python3.10/site-packages/datasets/arrow_dataset.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2262a8-1433-4327-affb-e3e1127dadee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
